{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from scipy.io import loadmat,savemat\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import *\n",
        "from keras import models\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fonctions utiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meUGvDclQTYN"
      },
      "outputs": [],
      "source": [
        "# Cette fonction permettra plus tard de charger plus ou moins d'images (en modifiant le paramètre num_images)\n",
        "# et de modifier la dimension d'entrée\n",
        "def load_data(image_size, num_images,path):\n",
        "  dirs = sorted(os.listdir(path))\n",
        "\n",
        "  x = np.zeros((min(num_images,len(dirs)),image_size,image_size))\n",
        "  y = np.zeros((min(num_images,len(dirs)), 3))\n",
        "    \n",
        "  #Chargement des normals    \n",
        "  mat_contents = loadmat(path+'/normals.mat')\n",
        "  normals = mat_contents['normals']\n",
        "  normals =tf.squeeze(normals)\n",
        "  #print(normals)\n",
        "\n",
        "  # Chargement des images, qui sont rangées dans lsp/images\n",
        "  for i in range(min(num_images,len(dirs))):\n",
        "    item = dirs[i]\n",
        "    #print(item)\n",
        "    if os.path.isfile(path+item):\n",
        "      img = Image.open(path+item)\n",
        "      # Redimensionnement et sauvegarde des normals\n",
        "      y[i][0] = normals[i][0]\n",
        "      y[i][1] = normals[i][1]\n",
        "      y[i][2] = normals[i][2]\n",
        "      # Redimensionnement et sauvegarde des images        \n",
        "      # img = img.resize((image_size,image_size))\n",
        "      x[i] = np.asarray(img)/255\n",
        "    \n",
        "  return x, y\n",
        "\n",
        "def plot_training_analysis(history):\n",
        "  acc = history.history['mae']\n",
        "  val_acc = history.history['val_mae']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "  \n",
        "  plt.plot(epochs, acc, 'b', linestyle=\"--\",label='Training mae')\n",
        "  plt.plot(epochs, val_acc, 'g', label='Validation mae')\n",
        "  plt.title('Training and validation mae')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training loss')\n",
        "  plt.plot(epochs, val_loss,'g', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unit_vector(vector):\n",
        "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
        "    return vector / np.linalg.norm(vector)\n",
        "\n",
        "def angle_between(v1, v2):\n",
        "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
        "\n",
        "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
        "            1.5707963267948966\n",
        "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
        "            0.0\n",
        "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
        "            3.141592653589793\n",
        "    \"\"\"\n",
        "    v1_u = unit_vector(v1)\n",
        "    v2_u = unit_vector(v2)\n",
        "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
        "\n",
        "def angle_between2(v1, v2):\n",
        "    v2 = np.array([v2[0],v2[1],v1[2]]) # On rajoute la 3ème valeur (n_z)\n",
        "    v1_u = unit_vector(v1)\n",
        "    v2_u = unit_vector(v2)\n",
        "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
        "\n",
        "def angle_error(y_true,y_pred):\n",
        "    angles = np.array(list(map(angle_between,y_true,y_pred)))\n",
        "    error = np.mean(angles)\n",
        "    return error\n",
        "\n",
        "def angle_error2(y_true,y_pred):\n",
        "    angles = np.array(list(map(angle_between2,y_true,y_pred)))\n",
        "    error = np.mean(angles)\n",
        "    return error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPaZp1EKPEo"
      },
      "source": [
        "## imagettes 3x3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On charge les données du peaks normal (qui vont devenir nos données de test)  \n",
        "x = imagettes, y = normales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2304, 3, 3) (2304, 3)\n"
          ]
        }
      ],
      "source": [
        "mat_contents = loadmat('Data/peaks_normal3.mat')\n",
        "x_test3 = mat_contents['imagettes']/255\n",
        "y_test3 = mat_contents['normals']\n",
        "print(x_test3.shape,y_test3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On charge les données des peaks un peu modifiés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11520, 3, 3) (11520, 3)\n"
          ]
        }
      ],
      "source": [
        "mat_contents = loadmat('Data/peaks_modified3.mat')\n",
        "x3 = mat_contents['imagettes']/255\n",
        "y3 = mat_contents['normals']\n",
        "print(x3.shape,y3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On sépare les données des peaks modifiés en 2 groupes (données de training et données de validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj94DujZPbwI",
        "outputId": "73e2866f-4692-4e3f-a2fe-07334120267f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train3,x_val3,y_train3,y_val3=train_test_split(x3,y3,test_size=0.1,random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On affiche certaines imagettes ainsi que leurs normales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5,5)\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(15)\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        axs[i,j].axis(\"off\")\n",
        "        axs[i,j].set_title(np.round(y_train3[i*5+j],2))\n",
        "        axs[i,j].imshow(x_train3[i*5+j],cmap='gray',vmin=0,vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Création et entraînement du réseau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dh54O9U89eMb",
        "outputId": "7312a2d1-0894-41e9-f37d-daa7efd5e967"
      },
      "outputs": [],
      "source": [
        "# Note : les boucles for ne sont plus utiles, on n'utilise pas la régularisation (val_l1 et val_l2 = 0) car les résultats étaient moins bons en utilisant la régularisation\n",
        "\n",
        "val_l1= [0]\n",
        "val_l2 = [0]\n",
        "historys = [0 for i in range(len(val_l2)*len(val_l1))]\n",
        "for (i,l2) in enumerate(val_l2) :\n",
        "    for (j,l1) in enumerate(val_l1):\n",
        "        print(\"Entraînement n°\"+str(len(val_l1)*i+j+1)+\"/\"+str(len(val_l2)*len(val_l1)))\n",
        "\n",
        "        regularizer = regularizers.l1_l2(l1=l1,l2=l2)\n",
        "\n",
        "        # Création du réseau\n",
        "        model9 = models.Sequential()\n",
        "        model9.add(Flatten())\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(32,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(16,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(8,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(3,activation='linear'))\n",
        "\n",
        "        # On utilise Adam, un optimiseur simple\n",
        "        opt = keras.optimizers.Adam(learning_rate=3e-3) \n",
        "\n",
        "        # On utilise une loss mean square error que l'on va chercher à minimiser, c'est grâce à cette loss que les poids vont être mis à jour à chaque batch\n",
        "        # On utilise comme métrique que l'on peut observer au cour de l'entraînement la mean absolute error\n",
        "        model9.compile(loss='mse',\n",
        "                    optimizer=opt,\n",
        "                    metrics='mae')\n",
        "\n",
        "        mcp_save = ModelCheckpoint('model_weights/meilleur_modele3.hdf5', save_best_only=True, monitor='val_mae', mode='min')\n",
        "\n",
        "        historys[len(val_l1)*i+j] = model9.fit(x_train3, y_train3,\n",
        "                epochs=100,\n",
        "                validation_data = (x_val3,y_val3),\n",
        "                batch_size=512,verbose=1,\n",
        "                callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "4xuoq3_MCa3R",
        "outputId": "688447e7-67db-4324-9717-ddc81285a626"
      },
      "outputs": [],
      "source": [
        "plot_training_analysis(historys[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On teste alors sur les données du peaks normal, que le réseau n'a jamais vu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model9 = models.load_model('model_weights/meilleur_modele3.hdf5')\n",
        "model9.evaluate(x_test3,y_test3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut également calculer l'erreur angulaire moyenne avec ces données de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred3=model9.predict(x_test3)\n",
        "angle_error(y_test3,y_pred3)*180/pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ou bien calculer tout cela sur les données d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model9.evaluate(x_train3,y_train3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred3=model9.predict(x_train3)\n",
        "angle_error(y_train3,y_pred3)*180/pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# La suite est identique mais pour des imagettes 9x9 et 13x13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-cDri_7Kh1D"
      },
      "source": [
        "# imagettes 9x9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_9x9_peaks = \"Data/train_data/imagettes_9x9_100/\"\n",
        "sz_9x9=9\n",
        "xp, yp = load_data(sz_9x9,8464,path_9x9_peaks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model9 = models.load_model('.mdl9_wts015_new.hdf5')\n",
        "model9 = models.load_model('.mdl9_wts.hdf5')\n",
        "model9.evaluate(xp,yp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yp_pred = model9.predict(xp)\n",
        "angle_error2(yp,yp_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1aqaAJXKqvo"
      },
      "outputs": [],
      "source": [
        "# path_9x9 = \"Data/train_data/imagettes_9x9_100/\"\n",
        "path_9x9_new = \"Data/train_data/imagettes_9x9_100_bis/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJhXNNG8HIs7"
      },
      "outputs": [],
      "source": [
        "# Chargement de seulement 10 images\n",
        "sz_9x9=9\n",
        "xx, yy = load_data(sz_9x9,1080,path_9x9_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalisation des données\n",
        "mean = xx.mean(axis=(0,1,2), keepdims=True)\n",
        "std = xx.std(axis=(0,1,2), keepdims=True)\n",
        "xx = (xx - mean)/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat_contents = loadmat('Data/peaks_normal9_100.mat')\n",
        "xp = mat_contents['imagettes']/255\n",
        "yp = mat_contents['normals']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model9 = models.load_model(\".mdl9_wts.hdf5\")\n",
        "model9.evaluate(xp,yp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yp_pred = model9.predict(xp)\n",
        "angle_error2(yp,yp_pred)*180/pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat_contents = loadmat('Data/peaks_modified9.mat')\n",
        "xx = mat_contents['imagettes']/255\n",
        "yy = mat_contents['normals']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj94DujZPbwI",
        "outputId": "73e2866f-4692-4e3f-a2fe-07334120267f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xx_train,xx_test,yy_train,yy_test=train_test_split(xx,yy,test_size=0.1,random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5,5)\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(15)\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        axs[i,j].axis(\"off\")\n",
        "        axs[i,j].set_title(np.round(yy_train[i*5+j],2))\n",
        "        axs[i,j].imshow(xx_train[i*5+j],cmap='gray',vmin=0,vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def my_linspace(start,num,step) :\n",
        "    return np.arange(0,num)*step+start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4h0q10APkGW",
        "outputId": "8661a7de-0538-4d43-ed50-f063f8dd2a3f"
      },
      "outputs": [],
      "source": [
        "val_l1= [0]\n",
        "val_l2 = [0]\n",
        "historys = [0 for i in range(len(val_l2)*len(val_l1))]\n",
        "for (i,l2) in enumerate(val_l2) :\n",
        "    for (j,l1) in enumerate(val_l1):\n",
        "        print(\"Entraînement n°\"+str(len(val_l1)*i+j+1)+\"/\"+str(len(val_l2)*len(val_l1)))\n",
        "\n",
        "        regularizer = regularizers.l1_l2(l1=l1,l2=l2)\n",
        "\n",
        "        model9 = models.Sequential()\n",
        "        model9.add(Flatten())\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(64,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(32,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(16,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(8,activation='relu',kernel_regularizer=regularizer))\n",
        "        model9.add(Dense(3,activation='linear'))\n",
        "        # model9 = models.load_model('.mdl9_wts.hdf5')\n",
        "\n",
        "        # On utilise Adam, un optimiseur simple, et l'on minimise une entropie croisée binaire\n",
        "        opt = keras.optimizers.Adam(learning_rate=3e-3) \n",
        "        #Loss= tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "        model9.compile(loss='mse',\n",
        "                    optimizer=opt,\n",
        "                    metrics='mae')\n",
        "\n",
        "        earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
        "        mcp_save = ModelCheckpoint('.mdl9_wts.hdf5', save_best_only=True, monitor='val_mae', mode='min')\n",
        "        # reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "        historys[len(val_l1)*i+j] = model9.fit(xx_train, yy_train,\n",
        "                epochs=500,\n",
        "                validation_data = (xx_test,yy_test),\n",
        "                batch_size=512,verbose=1,\n",
        "                callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "WNZ0UlyyPoY_",
        "outputId": "c87a8160-69f0-48f3-d06a-9ee98a0de53e"
      },
      "outputs": [],
      "source": [
        "plot_training_analysis(historys[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ5b2LRyClz-"
      },
      "outputs": [],
      "source": [
        "model9 = models.load_model('.mdl9_wts.hdf5')\n",
        "model9.evaluate(xp,yp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(historys)):\n",
        "    l1 = val_l1[i%len(val_l1)]\n",
        "    l2 = val_l2[i//len(val_l1)]\n",
        "    history = historys[i]\n",
        "    val_mae = history.history['val_mae']\n",
        "    print(\"val MAE min : \"+str(round(min(val_mae),2))+\" || l2=\"+str(l2)+\" l1=\"+str(l1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yy_pred = model9.predict(xp)\n",
        "angle_error(yp,yy_pred)*180/pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mae min : 0.15\n",
        "# angle_error : 0.33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scatter_data(x_val,y_val,model): \n",
        "  \"\"\"\n",
        "  plot le nuage de points \n",
        "  input : x_val :  imagettes de validation \n",
        "      y_val: noarmals associés\n",
        "      model: le model utilisé\n",
        "  output  : le plot de predictions=f(y_val)\n",
        "\n",
        "  \"\"\"\n",
        "  test_predictions = model.predict(x_val)\n",
        "  test_labels = y_val\n",
        "  fig, ax = plt.subplots(figsize=(8,4))\n",
        "  plt.scatter(test_labels, test_predictions, alpha=0.6, \n",
        "              color='#FF0000', lw=1, ec='black')\n",
        "  lims = [0, 1]\n",
        "\n",
        "  plt.plot(lims, lims, lw=1, color='#0000FF')\n",
        "  plt.axis(\"off\")\n",
        "  plt.ticklabel_format(useOffset=False, style='plain')\n",
        "  plt.xticks(fontsize=18)\n",
        "  plt.yticks(fontsize=18)\n",
        "  plt.xlim(lims)\n",
        "  plt.ylim(lims)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "scatter_data(xp,yp,model9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model9.evaluate(xp,yp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model9 = models.load_model('.mdl9_wts.hdf5')\n",
        "yp_pred = model9.predict(xp)\n",
        "angle_error(yp,yp_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yp_pred_rs = np.reshape(yp_pred,(42,42,3))\n",
        "yp_rs = np.reshape(yp,(42,42,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_dict = {\"normals_peaks9\":yp_pred_rs,\"normals_true9\":yp_rs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "savemat(\"normals_peaks9.mat\",y_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-cDri_7Kh1D"
      },
      "source": [
        "# imagettes 13x13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1aqaAJXKqvo"
      },
      "outputs": [],
      "source": [
        "path_13x13 = \"Data/train_data/imagettes_13x13/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJhXNNG8HIs7"
      },
      "outputs": [],
      "source": [
        "sz_13x13=13\n",
        "x13, y13 = load_data(sz_13x13,1444,path_13x13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj94DujZPbwI",
        "outputId": "73e2866f-4692-4e3f-a2fe-07334120267f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x13_train,x13_test,y13_train,y13_test=train_test_split(x13,y13,test_size=0.2,random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def my_linspace(start,num,step) :\n",
        "    return np.arange(0,num)*step+start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4h0q10APkGW",
        "outputId": "8661a7de-0538-4d43-ed50-f063f8dd2a3f"
      },
      "outputs": [],
      "source": [
        "l1=0\n",
        "l2=0\n",
        "\n",
        "model13 = models.Sequential()\n",
        "model13.add(Conv1D(128,3,activation='relu'))\n",
        "model13.add(Conv1D(32,3,activation='relu'))\n",
        "model13.add(Conv1D(16,3,activation='relu'))\n",
        "model13.add(Flatten())\n",
        "model13.add(Dense(16,activation='relu',kernel_regularizer=regularizer))\n",
        "model13.add(Dense(8,activation='relu',kernel_regularizer=regularizer))\n",
        "model13.add(Dense(3,activation='linear'))\n",
        "\n",
        "# On utilise Adam, un optimiseur simple, et l'on minimise une entropie croisée binaire\n",
        "opt = keras.optimizers.Adam(learning_rate=3e-3) \n",
        "\n",
        "model13.compile(loss='mse',\n",
        "              optimizer=opt,\n",
        "              metrics='mae')\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('.mdl13_wts.hdf5', save_best_only=True, monitor='val_mae', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "history = model13.fit(x13_train, y13_train,\n",
        "          epochs=1000,\n",
        "          validation_data = (x13_test, y13_test),\n",
        "          batch_size=50,verbose=1,\n",
        "          callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "WNZ0UlyyPoY_",
        "outputId": "c87a8160-69f0-48f3-d06a-9ee98a0de53e"
      },
      "outputs": [],
      "source": [
        "plot_training_analysis(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ5b2LRyClz-"
      },
      "outputs": [],
      "source": [
        "model13 = models.load_model('.mdl13_wts.hdf5')\n",
        "model13.evaluate(x13_test,y13_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y13_pred = model13.predict(x13_test)\n",
        "angle_error(y13_test,y13_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mae min : 0.16\n",
        "# angle_error : 0.34"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SfSNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
